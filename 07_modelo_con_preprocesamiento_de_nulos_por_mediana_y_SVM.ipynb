{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7GZqdgdEK7rB",
        "9MWFKQdWNZ83",
        "itmS6Di4nyVH",
        "tXuj2gshoFY9",
        "RpYfnRQCob7i",
        "NV609Yx1oejG",
        "_Nnf51a4pM1H",
        "vTaXTKr8oifM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianCarvajal/ProyectoIA/blob/main/07_modelo_con_preprocesamiento_de_nulos_por_mediana_y_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "NhbfrIp4KlCQ",
        "outputId": "d9eee5ac-8d28-413d-f957-3af9868876f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<meta name=\"google-signin-client_id\"\n",
              "      content=\"461673936472-kdjosv61up3ac1ajeuq6qqu72upilmls.apps.googleusercontent.com\"/>\n",
              "<script src=\"https://apis.google.com/js/client:platform.js?onload=google_button_start\"></script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replicating local resources\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>See <a href='https://m5knaekxo6.execute-api.us-west-2.amazonaws.com/dev-v0001/rlxmooc/web/login' target='_blank'>my courses and progress</a></h2>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/ai4eng.v1/main/content/init.py\n",
        "import init; init.init(force_download=False); init.get_weblink()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4H-zDqDWB3r",
        "outputId": "5bdce0e5-1d54-4d0a-fc98-c57927052cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/235.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhRaVMI-v0re",
        "outputId": "41ad86ea-ffd9-462f-f53c-668b0702c214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download data directly from Kaggle\n",
        "\n",
        "- create a file `kaggle.json` with your authentication token (in kaggle $\\to$ click user icon on top-right $\\to$ settings $\\to$ API create new token)\n",
        "- upload it to this notebook workspace\n",
        "- run the following cell"
      ],
      "metadata": {
        "id": "7GZqdgdEK7rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20251-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Px7YBcK3OY",
        "outputId": "2173dc0a-ed57-4a4f-9477-26a2ecb752d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading udea-ai-4-eng-20251-pruebas-saber-pro-colombia.zip to /content\n",
            "\r  0% 0.00/29.9M [00:00<?, ?B/s]\n",
            "\r100% 29.9M/29.9M [00:00<00:00, 418MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unzip and inspect data"
      ],
      "metadata": {
        "id": "9MWFKQdWNZ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea*.zip > /dev/null"
      ],
      "metadata": {
        "id": "Q0D1i3q9NaaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I7QgZbBNdNy",
        "outputId": "f33f1d69-0a8e-4bf4-d785-56eb37e6d667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   296787    296787   4716673 submission_example.csv\n",
            "   296787   4565553  59185250 test.csv\n",
            "   692501  10666231 143732449 train.csv\n",
            "  1286075  15528571 207634372 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "itmS6Di4nyVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('train.csv', encoding = 'utf-8')\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFNqTXp0NgJI",
        "outputId": "cff8b1cc-29df-48a0-9efa-36a919db87ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(692500, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones reutilizables"
      ],
      "metadata": {
        "id": "-nbCybnoucwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "def clean_text(colum_name):\n",
        "  df[colum_name] = (\n",
        "      df[colum_name]\n",
        "      .astype(str)                            # Asegura que todo sea texto\n",
        "      .str.lower()                            # Convierte a minúsculas\n",
        "      .str.strip()                            # Quita espacios al principio/final\n",
        "      .apply(unidecode)                       # Elimina tildes\n",
        "      .str.replace(r'\\s+', ' ', regex=True)   # Sustituye múltiples espacios por uno solo\n",
        "  )\n"
      ],
      "metadata": {
        "id": "8BgR247-ug5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "# --- 1. Función de Pre-procesamiento Robusto (Sin cambios, ya está en la función de limpieza) ---\n",
        "def preprocesar_texto(texto):\n",
        "    texto = str(texto).lower()\n",
        "    texto = unidecode(texto)\n",
        "    texto = re.sub(r'[^a-z0-9\\s]', '', texto) # Esto maneja el '?' y otros especiales\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "# --- 2. Función de Limpieza de Columna con Fuzzy Matching (MEJORADA CON DEBUG) ---\n",
        "def limpiar_columna_fuzzy_debug(df, columna_a_limpiar, umbral=85):\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # Aplicar el pre-procesamiento a una nueva columna temporal\n",
        "    columna_preprocesada = f\"{columna_a_limpiar}_preprocesada\"\n",
        "    df_temp[columna_preprocesada] = df_temp[columna_a_limpiar].apply(preprocesar_texto)\n",
        "\n",
        "    # Obtener los programas únicos de la columna pre-procesada\n",
        "    programas_unicos_preproc = df_temp[columna_preprocesada].unique().tolist()\n",
        "\n",
        "    mapeo_programas_preproc_a_canonico = {}\n",
        "    programas_preproc_procesados = set()\n",
        "\n",
        "    # Iterar sobre cada programa único pre-procesado\n",
        "    for i, programa_actual_preproc in enumerate(programas_unicos_preproc):\n",
        "        if programa_actual_preproc in programas_preproc_procesados:\n",
        "            continue\n",
        "\n",
        "        nombre_canonico_preproc = programa_actual_preproc\n",
        "        mapeo_programas_preproc_a_canonico[programa_actual_preproc] = nombre_canonico_preproc\n",
        "        programas_preproc_procesados.add(programa_actual_preproc)\n",
        "        print(f\"\\n[{i+1}/{len(programas_unicos_preproc)}] Estableciendo canónico: '{nombre_canonico_preproc}'\")\n",
        "\n",
        "        # Busca coincidencias entre este canónico y los programas que aún no se han mapeado\n",
        "        opciones_a_comparar = [p for p in programas_unicos_preproc if p not in programas_preproc_procesados]\n",
        "\n",
        "        if not opciones_a_comparar:\n",
        "            print(f\"  No hay más opciones para comparar con '{nombre_canonico_preproc}'.\")\n",
        "            continue\n",
        "\n",
        "        coincidencias = process.extract(\n",
        "            query=programa_actual_preproc,\n",
        "            choices=opciones_a_comparar,\n",
        "            scorer=fuzz.token_set_ratio,\n",
        "            limit=None\n",
        "        )\n",
        "\n",
        "        found_match_in_group = False\n",
        "        for posible_variacion_preproc, puntuacion, _ in coincidencias:\n",
        "            if puntuacion >= umbral:\n",
        "                if posible_variacion_preproc not in programas_preproc_procesados:\n",
        "                    mapeo_programas_preproc_a_canonico[posible_variacion_preproc] = nombre_canonico_preproc\n",
        "                    programas_preproc_procesados.add(posible_variacion_preproc)\n",
        "                    print(f\"  -> Mapeando '{posible_variacion_preproc}' (score: {puntuacion}) a '{nombre_canonico_preproc}'\")\n",
        "                    found_match_in_group = True\n",
        "            # DEBUG: Mostrar coincidencias cercanas pero que no cumplen el umbral\n",
        "            elif puntuacion > umbral - 15: # Mostrar si la puntuación es ~15 puntos por debajo del umbral\n",
        "                 print(f\"  -> IGNORANDO '{posible_variacion_preproc}' (score: {puntuacion}) para '{nombre_canonico_preproc}' - Puntuación muy baja.\")\n",
        "\n",
        "        if not found_match_in_group and len(opciones_a_comparar) > 0:\n",
        "            print(f\"  No se encontraron coincidencias por encima del umbral={umbral} para '{nombre_canonico_preproc}' en las {len(opciones_a_comparar)} opciones restantes.\")\n",
        "\n",
        "\n",
        "    # --- Paso para obtener la forma ORIGINAL del nombre canónico ---\n",
        "    # Crear un mapeo inverso de la forma preprocesada canónica a una de sus formas originales\n",
        "    canonical_preproc_a_original_map = {}\n",
        "    for original_val, preprocessed_val in df_temp[[columna_a_limpiar, columna_preprocesada]].drop_duplicates().values:\n",
        "        if preprocessed_val in mapeo_programas_preproc_a_canonico:\n",
        "            cleaned_preprocessed_val = mapeo_programas_preproc_a_canonico[preprocessed_val]\n",
        "            if cleaned_preprocessed_val not in canonical_preproc_a_original_map:\n",
        "                canonical_preproc_a_original_map[cleaned_preprocessed_val] = original_val\n",
        "        else:\n",
        "            # En casos muy raros donde un preprocesado no fue mapeado (debería ser handled por el 'else' del bucle)\n",
        "            if preprocessed_val not in canonical_preproc_a_original_map:\n",
        "                canonical_preproc_a_original_map[preprocessed_val] = original_val\n",
        "\n",
        "    # Aplicar este mapeo para obtener la columna final limpia con nombres originales\n",
        "    df_temp[f'{columna_a_limpiar}_limpio'] = df_temp[f'{columna_a_limpiar}_preprocesada'].map(mapeo_programas_preproc_a_canonico).map(canonical_preproc_a_original_map)\n",
        "\n",
        "    # Eliminar las columnas temporales preprocesadas\n",
        "    df_final = df_temp.drop(columns=[columna_preprocesada]) # Ya no necesitamos la preproc, ni la limpia_preproc\n",
        "\n",
        "    # Construir el diccionario de mapeo final (original -> original_limpio) para el retorno\n",
        "    final_mapeo_para_retornar = {}\n",
        "    for original_val in df[columna_a_limpiar].unique():\n",
        "        preprocessed_val = preprocesar_texto(original_val)\n",
        "        if preprocessed_val in mapeo_programas_preproc_a_canonico:\n",
        "            cleaned_preprocessed_val = mapeo_programas_preproc_a_canonico[preprocessed_val]\n",
        "            final_mapeo_para_retornar[original_val] = canonical_preproc_a_original_map.get(cleaned_preprocessed_val, original_val)\n",
        "        else:\n",
        "            # Fallback en caso de que un valor original no se haya mapeado (raro)\n",
        "            final_mapeo_para_retornar[original_val] = original_val\n",
        "\n",
        "    num_unicos_final = len(df_final[f'{columna_a_limpiar}_limpio'].unique())\n",
        "    # print(\"\\n----------------------------------------------------------------------\")\n",
        "    # print(f\"Número de programas únicos DESPUÉS de limpieza: {num_unicos_final}\")\n",
        "    # if num_unicos_final == len(programas_unicos_preproc):\n",
        "    #     print(\"ADVERTENCIA: No se realizó ninguna agrupación. Considera bajar el umbral o revisar tus datos.\")\n",
        "    # print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "    return df_final, final_mapeo_para_retornar"
      ],
      "metadata": {
        "id": "czfdv3UQzApn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "8DhFydTSn68d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codificación de variables categóricas"
      ],
      "metadata": {
        "id": "tXuj2gshoFY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rendimiento"
      ],
      "metadata": {
        "id": "RpYfnRQCob7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_performance(df):\n",
        "  maped_performance_values = {'bajo': 1, 'medio-bajo':2, 'medio-alto':3, 'alto':4}\n",
        "  df['RENDIMIENTO_GLOBAL'] = df['RENDIMIENTO_GLOBAL'].map(maped_performance_values)\n",
        "  return df['RENDIMIENTO_GLOBAL']"
      ],
      "metadata": {
        "id": "6SXRTwYkdup1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Familia con internet"
      ],
      "metadata": {
        "id": "NV609Yx1oejG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_internet(df):\n",
        "  maped_internet_values = {'Si': 1, 'No':0}\n",
        "  df['FAMI_TIENEINTERNET'] = df['FAMI_TIENEINTERNET'].map(maped_internet_values)"
      ],
      "metadata": {
        "id": "pZb-0LdewijL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Familia con internet (Columna repetida con \"si\" y \"no\" cómo respuesta)"
      ],
      "metadata": {
        "id": "_Nnf51a4pM1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_internet_1(df):\n",
        "  del(df['FAMI_TIENEINTERNET.1'])"
      ],
      "metadata": {
        "id": "6eg0HaQOEI7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Valor matrícula"
      ],
      "metadata": {
        "id": "vTaXTKr8oifM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_matr(df):\n",
        "  maped_tuition_values = {'No pagó matrícula': 0,\n",
        "                          'Menos de 500 mil':1,\n",
        "                          'Entre 500 mil y menos de 1 millón':2,\n",
        "                          'Entre 1 millón y menos de 2.5 millones':3,\n",
        "                          'Entre 2.5 millones y menos de 4 millones':4,\n",
        "                          'Entre 4 millones y menos de 5.5 millones':5,\n",
        "                          'Entre 5.5 millones y menos de 7 millones':6,\n",
        "                          'Más de 7 millones':7}\n",
        "  df['ESTU_VALORMATRICULAUNIVERSIDAD'] = df['ESTU_VALORMATRICULAUNIVERSIDAD'].map(maped_tuition_values)"
      ],
      "metadata": {
        "id": "09wIGgcy1ojm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Horas de trabajo semanales"
      ],
      "metadata": {
        "id": "mDe0amw9omjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_work_hours(df):\n",
        "  maped_working_values = {'0': 0,\n",
        "                          'Menos de 10 horas':1,\n",
        "                          'Entre 11 y 20 horas':2,\n",
        "                          'Entre 21 y 30 horas':3,\n",
        "                          'Más de 30 horas':4}\n",
        "  df['ESTU_HORASSEMANATRABAJA'] = df['ESTU_HORASSEMANATRABAJA'].map(maped_working_values)"
      ],
      "metadata": {
        "id": "plbfP0Mk4Cwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estrato"
      ],
      "metadata": {
        "id": "LXXz_IjrorVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_stratum(df):\n",
        "  maped_estrato_values = {'0': 0,\n",
        "                          'Estrato 1':1,\n",
        "                          'Estrato 2':2,\n",
        "                          'Estrato 3':3,\n",
        "                          'Estrato 4':4,\n",
        "                          'Estrato 5':5,\n",
        "                          'Estrato 6':6}\n",
        "  df['FAMI_ESTRATOVIVIENDA'] = df['FAMI_ESTRATOVIVIENDA'].map(maped_estrato_values)"
      ],
      "metadata": {
        "id": "rsELECsl4ZiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Educación padre"
      ],
      "metadata": {
        "id": "BGEfh3hVovcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_p_education(df):\n",
        "  maped_education_values = {'Ninguno': 0,\n",
        "                          'Primaria incompleta':1,\n",
        "                          'Primaria completa':2,\n",
        "                          'Secundaria (Bachillerato) incompleta':3,\n",
        "                          'Secundaria (Bachillerato) completa':4,\n",
        "                          'Técnica o tecnológica incompleta':5,\n",
        "                          'Técnica o tecnológica completa':6,\n",
        "                          'Educación profesional incompleta':7,\n",
        "                          'Educación profesional completa':8,\n",
        "                          'Postgrado':9}\n",
        "  df['FAMI_EDUCACIONPADRE'] = df['FAMI_EDUCACIONPADRE'].map(maped_education_values)"
      ],
      "metadata": {
        "id": "cuv3IB7JAnYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Educación madre"
      ],
      "metadata": {
        "id": "hRdyNFFrpXnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_m_education(df):\n",
        "  maped_education_values = {'Ninguno': 0,\n",
        "                          'Primaria incompleta':1,\n",
        "                          'Primaria completa':2,\n",
        "                          'Secundaria (Bachillerato) incompleta':3,\n",
        "                          'Secundaria (Bachillerato) completa':4,\n",
        "                          'Técnica o tecnológica incompleta':5,\n",
        "                          'Técnica o tecnológica completa':6,\n",
        "                          'Educación profesional incompleta':7,\n",
        "                          'Educación profesional completa':8,\n",
        "                          'Postgrado':9}\n",
        "  df['FAMI_EDUCACIONMADRE'] = df['FAMI_EDUCACIONMADRE'].map(maped_education_values)"
      ],
      "metadata": {
        "id": "UdU2jAziEghL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tiene lavadora"
      ],
      "metadata": {
        "id": "eJomTbpDoxso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_washing_machine(df):\n",
        "  maped_washing_values = {'No': 0,\n",
        "                          'Si': 1}\n",
        "  df['FAMI_TIENELAVADORA'] = df['FAMI_TIENELAVADORA'].map(maped_washing_values)"
      ],
      "metadata": {
        "id": "wb1DDgiYBVM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tiene automovil"
      ],
      "metadata": {
        "id": "2-ptDULoo0LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_car(df):\n",
        "  maped_car_values = {'No': 0,\n",
        "                      'Si': 1}\n",
        "  df['FAMI_TIENEAUTOMOVIL'] = df['FAMI_TIENEAUTOMOVIL'].map(maped_car_values)"
      ],
      "metadata": {
        "id": "qnhw45niB6aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Privado de la libertad"
      ],
      "metadata": {
        "id": "YZDAs6R8o3Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_priv_lib(df):\n",
        "  maped_estu_priv_values = {'N': 0,\n",
        "                      'S': 1}\n",
        "  df['ESTU_PRIVADO_LIBERTAD'] = df['ESTU_PRIVADO_LIBERTAD'].map(maped_estu_priv_values)"
      ],
      "metadata": {
        "id": "GztcKgHxCZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Se paga su matrícula"
      ],
      "metadata": {
        "id": "lnq9_y2NpA0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_own_pay(df):\n",
        "  maped_ownpay_values = {'No': 0,\n",
        "                      'Si': 1}\n",
        "  df['ESTU_PAGOMATRICULAPROPIO'] = df['ESTU_PAGOMATRICULAPROPIO'].map(maped_ownpay_values)"
      ],
      "metadata": {
        "id": "bQY1nmjLDfb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tiene computador"
      ],
      "metadata": {
        "id": "MC-vs8LtpFn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_own_pc(df):\n",
        "  maped_ownpc_values = {'No': 0,\n",
        "                      'Si': 1}\n",
        "  df['FAMI_TIENECOMPUTADOR'] = df['FAMI_TIENECOMPUTADOR'].map(maped_ownpc_values)"
      ],
      "metadata": {
        "id": "x4wGDRBlDuHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codificación One-hot encoding para variables categóricas no ordinales"
      ],
      "metadata": {
        "id": "ok5eLnsr-ptc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dpto(df):\n",
        "  df['ESTU_PRGM_DEPARTAMENTO'] = df['ESTU_PRGM_DEPARTAMENTO'].str.strip().str.upper()\n",
        "\n",
        "  df_aux = pd.get_dummies(df,\n",
        "                              columns=['ESTU_PRGM_DEPARTAMENTO'],\n",
        "                              prefix='DEPTO',\n",
        "                              prefix_sep='_',\n",
        "                              dtype='uint8')\n",
        "  return df_aux"
      ],
      "metadata": {
        "id": "OCG8ItEB_Kkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_periodo(df):\n",
        "  df_aux = pd.get_dummies(df,\n",
        "                              columns=['PERIODO'],\n",
        "                              prefix='PERIODO',\n",
        "                              prefix_sep='_',\n",
        "                              dtype='uint8')\n",
        "  return df_aux"
      ],
      "metadata": {
        "id": "lL9SxuEXdHHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def clean_program(df):\n",
        "  # Paso 1: Crear una función de limpieza para normalizar los nombres de los programas.\n",
        "  def limpiar_texto(texto):\n",
        "      if not isinstance(texto, str):\n",
        "          return texto\n",
        "\n",
        "      texto = ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
        "      texto = texto.lower().strip().replace('-', ' ').replace('.', '').replace(',', '')\n",
        "      return ' '.join(texto.split())\n",
        "\n",
        "  # Paso 2: Aplicar la limpieza para crear una columna de trabajo temporal.\n",
        "  df['ESTU_PRGM_ACADEMICO_LIMPIO'] = df['ESTU_PRGM_ACADEMICO'].apply(limpiar_texto)\n",
        "  del(df['ESTU_PRGM_ACADEMICO'])\n",
        "  print(\"aaaaaaaaa\")\n",
        "  # Paso 3: Agrupar por frecuencia (mantenemos los 30 programas mas frecuentes).\n",
        "  N = 30\n",
        "  top_n_programas = df['ESTU_PRGM_ACADEMICO_LIMPIO'].value_counts().nlargest(N).index\n",
        "\n",
        "  # Creamos otra columna temporal con los programas agrupados.\n",
        "  df['PRGM_AGRUPADO_TEMP'] = df['ESTU_PRGM_ACADEMICO_LIMPIO'].where(df['ESTU_PRGM_ACADEMICO_LIMPIO'].isin(top_n_programas), 'OTRO')\n",
        "\n",
        "  # Paso 4: Aplicar One-Hot Encoding sobre la columna agrupada.\n",
        "  df = pd.get_dummies(df, columns=['PRGM_AGRUPADO_TEMP'], prefix='PRGM', dtype=int)\n",
        "\n",
        "  # Paso 5: Limpieza final. Eliminamos las columnas temporales que creamos.\n",
        "  # Las columnas originales no se tocan.\n",
        "  del(df['ESTU_PRGM_ACADEMICO_LIMPIO'])\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "J9TrRo-5ddCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamiento de datos nulos"
      ],
      "metadata": {
        "id": "C5cdjCDGeJgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_nulls(df):\n",
        "  columnas_numericas = [\n",
        "      'ESTU_VALORMATRICULAUNIVERSIDAD',\n",
        "      'ESTU_HORASSEMANATRABAJA'\n",
        "  ]\n",
        "\n",
        "  columnas_categoricas = [\n",
        "      'FAMI_ESTRATOVIVIENDA', 'FAMI_TIENEINTERNET', 'FAMI_EDUCACIONPADRE',\n",
        "      'FAMI_TIENELAVADORA', 'FAMI_TIENEAUTOMOVIL', 'ESTU_PAGOMATRICULAPROPIO',\n",
        "      'FAMI_TIENECOMPUTADOR', 'FAMI_EDUCACIONMADRE'\n",
        "  ]\n",
        "\n",
        "  # 1. Reemplazamos los valores nulos con la mediana\n",
        "  for col in columnas_numericas:\n",
        "      mediana = df[col].median()\n",
        "      df[col] = df[col].fillna(mediana)\n",
        "\n",
        "  # 2. Reemplazamos los valores nulos con la MODA\n",
        "  for col in columnas_categoricas:\n",
        "      moda = df[col].mode()[0]\n",
        "      df[col] = df[col].fillna(moda)"
      ],
      "metadata": {
        "id": "KgbEvmq5eLr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data ready for training"
      ],
      "metadata": {
        "id": "lPnst9kPpbCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  # clean_performance(df)\n",
        "  clean_internet(df)\n",
        "  clean_internet_1(df)\n",
        "  clean_matr(df)\n",
        "  clean_work_hours(df)\n",
        "  clean_stratum(df)\n",
        "  clean_p_education(df)\n",
        "  clean_m_education(df)\n",
        "  clean_washing_machine(df)\n",
        "  clean_car(df)\n",
        "  clean_priv_lib(df)\n",
        "  clean_own_pay(df)\n",
        "  clean_own_pc(df)\n",
        "  df = clean_dpto(df)\n",
        "  df = clean_periodo(df)\n",
        "  df = clean_program(df)\n",
        "  clean_nulls(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "GBThlDey2nG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos los datos de train y de test"
      ],
      "metadata": {
        "id": "iczjhTw_rQEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = pd.read_csv('train.csv', encoding = 'utf-8')\n",
        "dts = pd.read_csv('test.csv', encoding = 'utf-8')\n",
        "lentr = len(dtr)\n",
        "dtr.shape, dts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lcWzJ_N1Wbx",
        "outputId": "b8b4d4a0-f376-4d25-f744-25a1c6dff518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((692500, 21), (296786, 20))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Juntamos train y test con el objetivo de realizar limpieza a ambos datasets de una forma mas comoda"
      ],
      "metadata": {
        "id": "qIqpMDmgrTjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_cols = [i for i in dtr.columns if i != 'RENDIMIENTO_GLOBAL']\n",
        "all_data = pd.concat([dtr[source_cols], dts[source_cols]])\n",
        "all_data.index = range(len(all_data))\n",
        "all_data = clean_data(all_data)\n",
        "\n",
        "xtr, ytr = all_data.iloc[:lentr].values, clean_performance(dtr).values\n",
        "xts = all_data.iloc[lentr:].values\n",
        "\n",
        "print(xtr.shape, ytr.shape, xts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjqWqYyyhiXt",
        "outputId": "731112c0-4291-46ef-8780-027766a5964d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaaaaaaaa\n",
            "(692500, 87) (692500,) (296786, 87)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decidimos separar el dataframe de 'train' en una proporcion 70 - 30 con el objetivo de probar la calibracion del modelo con datos no vistos antes de realizar la prueba final dentro de kaggle para tener una aproximacion de que modelo con que pre-procesado funciona mejor"
      ],
      "metadata": {
        "id": "6pWtGKskrbhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xtr, xval, ytr, yval = train_test_split(xtr, ytr, test_size=0.3)\n",
        "xtr.shape, xval.shape, ytr.shape, yval.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftTdgC5UfUQf",
        "outputId": "98e7cb67-2416-481c-fba6-c21a56b7e675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((484750, 87), (207750, 87), (484750,), (207750,))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos un 'Support vector machine' con un gamma"
      ],
      "metadata": {
        "id": "xUuUc70Br_c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(gamma='auto')\n",
        "svc.fit(xtr, ytr)\n",
        "\n",
        "y_pred = svc.predict(xval)"
      ],
      "metadata": {
        "id": "JJ3ryM6o6A9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generar la matriz de confusión\n",
        "cm = confusion_matrix(yval, y_pred)\n",
        "\n",
        "# Visualizarla para que sea más fácil de entender\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicción del Modelo')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tqQy0YQQjeZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(yval, y_pred)\n",
        "print(f\"Precisión (Accuracy): {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4KbGiXwrjsQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}